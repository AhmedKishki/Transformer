{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from make_model import make_model\n",
    "from Processing.batch import Batch\n",
    "from Processing.labelsmoothing import LabelSmoothing\n",
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model(batch.src, batch.tgt, batch.src_mask, batch.tgt_mask)\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        \n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        \n",
    "        if i % 40 == 0 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            elapsed = time.time() - start\n",
    "            print((\"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f | Tokens / Sec: %7.1f | Learning Rate: %6.1e\")\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return (total_loss / total_tokens), train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, projection, criterion):\n",
    "        self.proj = projection\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = log_softmax(self.proj(x), dim=-1)\n",
    "        sloss = (self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm)\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "        \n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizers():\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de, spacy_en = load_tokenizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter, tokenizer, index):\n",
    "    for from_to_tuple in data_iter:\n",
    "        yield tokenizer(from_to_tuple[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    return tokenize(text, spacy_de)\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return tokenize(text, spacy_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary():\n",
    "    print(\"Building German Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_src = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val, tokenize_de, index=0),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    print(\"Building English Vocabulary ...\")\n",
    "    train, val, test = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "    vocab_tgt = build_vocab_from_iterator(\n",
    "        yield_tokens(train + val, tokenize_en, index=1),\n",
    "        min_freq=2,\n",
    "        specials=[\"<s>\", \"</s>\", \"<blank>\", \"<unk>\"],\n",
    "    )\n",
    "\n",
    "    vocab_src.set_default_index(vocab_src[\"<unk>\"])\n",
    "    vocab_tgt.set_default_index(vocab_tgt[\"<unk>\"])\n",
    "    \n",
    "    del train\n",
    "    del val\n",
    "    del test\n",
    "    \n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "def load_vocab():\n",
    "    if not exists(\"vocab.pt\"):\n",
    "        vocab_src, vocab_tgt = build_vocabulary()\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab.pt\")\n",
    "    print(\"Finished.\\nVocabulary sizes:\")\n",
    "    print(len(vocab_src))\n",
    "    print(len(vocab_tgt))\n",
    "    return vocab_src, vocab_tgt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "8185\n",
      "6291\n"
     ]
    }
   ],
   "source": [
    "vocab_src, vocab_tgt = load_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(\n",
    "    batch,\n",
    "    src_pipeline,\n",
    "    tgt_pipeline,\n",
    "    src_vocab,\n",
    "    tgt_vocab,\n",
    "    device='cuda',\n",
    "    max_padding=128,\n",
    "    pad_id=2,\n",
    "):\n",
    "    bs_id = torch.tensor([0], device=device)  # <s> token id\n",
    "    eos_id = torch.tensor([1], device=device)  # </s> token id\n",
    "    src_list, tgt_list = [], []\n",
    "    for (_src, _tgt) in batch:\n",
    "        processed_src = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    src_vocab(src_pipeline(_src)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    tgt_vocab(tgt_pipeline(_tgt)),\n",
    "                    dtype=torch.int64,\n",
    "                    device=device,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        src_list.append(\n",
    "            # warning - overwrites values for negative values of padding - len\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (\n",
    "                    0,\n",
    "                    max_padding - len(processed_src),\n",
    "                ),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_padding - len(processed_tgt)),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    src = torch.stack(src_list,)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "    return src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    batch_size=12000,\n",
    "    max_padding=128,\n",
    "    is_distributed=True,\n",
    "    device='cuda'\n",
    "):\n",
    "    # def create_dataloaders(batch_size=12000):\n",
    "    def collate_fn(batch):\n",
    "        return collate_batch(\n",
    "            batch,\n",
    "            tokenize_de,\n",
    "            tokenize_en,\n",
    "            vocab_src,\n",
    "            vocab_tgt,\n",
    "            device,\n",
    "            max_padding=max_padding,\n",
    "            pad_id=vocab_src.get_stoi()[\"<blank>\"],\n",
    "        )\n",
    "\n",
    "    train_iter, valid_iter, _ = datasets.Multi30k(language_pair=(\"de\", \"en\"))\n",
    "\n",
    "    train_iter_map = to_map_style_dataset(train_iter)  # DistributedSampler needs a dataset len()\n",
    "    train_sampler = (DistributedSampler(train_iter_map) if is_distributed else None)\n",
    "    valid_iter_map = to_map_style_dataset(valid_iter)\n",
    "    valid_sampler = (DistributedSampler(valid_iter_map) if is_distributed else None)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        sampler=train_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        generator=torch.Generator('cpu')\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_iter_map,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=(valid_sampler is None),\n",
    "        sampler=valid_sampler,\n",
    "        collate_fn=collate_fn,\n",
    "        generator=torch.Generator('cpu')\n",
    "    )\n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    gpu,\n",
    "    ngpus_per_node,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    config,\n",
    "    is_distributed=False,\n",
    "):\n",
    "    print(f\"Train worker process using cuda:{gpu} for training\", flush=True)\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    d_model = 512\n",
    "    model = make_model(config)\n",
    "    is_main_process = True\n",
    "    \n",
    "    # if is_distributed:\n",
    "    #     dist.init_process_group(\"nccl\", init_method=\"env://\", rank=gpu, world_size=ngpus_per_node)\n",
    "    #     model = DDP(model, device_ids=[gpu])\n",
    "    #     module = model.module\n",
    "    #     is_main_process = gpu == 0\n",
    "\n",
    "    criterion = LabelSmoothing(size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1)\n",
    "    criterion.to(device)\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        batch_size=config.batch_size // ngpus_per_node,\n",
    "        max_padding=config.max_padding,\n",
    "        is_distributed=is_distributed,\n",
    "        device='cuda'\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.base_lr, betas=(0.9, 0.98), eps=1e-9)\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config.warmup\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "\n",
    "    def gen_batch(dataloader):\n",
    "        for b0, b1 in dataloader:\n",
    "            yield Batch(b0.to(device), b1.to(device), pad_idx)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        # if is_distributed:\n",
    "        #     train_dataloader.sampler.set_epoch(epoch)\n",
    "        #     valid_dataloader.sampler.set_epoch(epoch)\n",
    "\n",
    "        model.train()\n",
    "        print(f\"Epoch {epoch} Training ====\", flush=True)\n",
    "        _, train_state = run_epoch(\n",
    "            gen_batch(train_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(model.proj, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config.accum_iter,\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config.file_prefix, epoch)\n",
    "            torch.save(model.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        sloss, _ = run_epoch(\n",
    "            gen_batch(valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(model.proj, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\", \n",
    "        )\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config.file_prefix\n",
    "        torch.save(model.state_dict(), file_path)\n",
    "\n",
    "# def train_distributed_model(vocab_src, vocab_tgt, config):\n",
    "#     ngpus = torch.cuda.device_count()\n",
    "#     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#     os.environ[\"MASTER_PORT\"] = \"12356\"\n",
    "#     print(f\"Number of GPUs detected: {ngpus}\")\n",
    "#     print(\"Spawning training processes ...\")\n",
    "#     mp.spawn(train_worker, nprocs=ngpus, args=(ngpus, vocab_src, vocab_tgt, config, True))\n",
    "\n",
    "def train_model(vocab_src, vocab_tgt, config):\n",
    "    # if config.distributed:\n",
    "    #     train_distributed_model(vocab_src, vocab_tgt, config)\n",
    "    # else:\n",
    "        train_worker(0, 1, vocab_src, vocab_tgt, config, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "config.batch_size = 32\n",
    "config.src_vocab = len(vocab_src)\n",
    "config.tgt_vocab = len(vocab_tgt)\n",
    "config.N = 6\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model():\n",
    "    model_path = \"multi30k_model_final.pt\"\n",
    "    if not exists(model_path):\n",
    "        train_model(vocab_src, vocab_tgt, config)\n",
    "    model = make_model(config).to(device)\n",
    "    model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train worker process using cuda:0 for training\n",
      "Epoch 0 Training ====\n",
      "Epoch Step:      0 | Accumulation Step:   1 | Loss:   7.64 | Tokens / Sec:   272.9 | Learning Rate: 2.7e-07\n",
      "Epoch Step:     40 | Accumulation Step:   5 | Loss:   7.54 | Tokens / Sec:   958.7 | Learning Rate: 1.1e-05\n",
      "Epoch Step:     80 | Accumulation Step:   9 | Loss:   7.12 | Tokens / Sec:   961.2 | Learning Rate: 2.2e-05\n",
      "Epoch Step:    120 | Accumulation Step:  13 | Loss:   6.74 | Tokens / Sec:   967.5 | Learning Rate: 3.3e-05\n",
      "Epoch Step:    160 | Accumulation Step:  17 | Loss:   6.53 | Tokens / Sec:   959.7 | Learning Rate: 4.3e-05\n",
      "Epoch Step:    200 | Accumulation Step:  21 | Loss:   6.30 | Tokens / Sec:   959.1 | Learning Rate: 5.4e-05\n",
      "Epoch Step:    240 | Accumulation Step:  25 | Loss:   6.09 | Tokens / Sec:   967.6 | Learning Rate: 6.5e-05\n",
      "Epoch Step:    280 | Accumulation Step:  29 | Loss:   6.09 | Tokens / Sec:   980.7 | Learning Rate: 7.6e-05\n",
      "Epoch Step:    320 | Accumulation Step:  33 | Loss:   5.78 | Tokens / Sec:   974.8 | Learning Rate: 8.6e-05\n",
      "Epoch Step:    360 | Accumulation Step:  37 | Loss:   5.58 | Tokens / Sec:   977.3 | Learning Rate: 9.7e-05\n",
      "Epoch Step:    400 | Accumulation Step:  41 | Loss:   5.32 | Tokens / Sec:   983.3 | Learning Rate: 1.1e-04\n",
      "Epoch Step:    440 | Accumulation Step:  45 | Loss:   5.02 | Tokens / Sec:   992.8 | Learning Rate: 1.2e-04\n",
      "Epoch Step:    480 | Accumulation Step:  49 | Loss:   4.93 | Tokens / Sec:   979.9 | Learning Rate: 1.3e-04\n",
      "Epoch Step:    520 | Accumulation Step:  53 | Loss:   4.68 | Tokens / Sec:   976.0 | Learning Rate: 1.4e-04\n",
      "Epoch Step:    560 | Accumulation Step:  57 | Loss:   4.38 | Tokens / Sec:   981.4 | Learning Rate: 1.5e-04\n",
      "Epoch Step:    600 | Accumulation Step:  61 | Loss:   4.48 | Tokens / Sec:   973.8 | Learning Rate: 1.6e-04\n",
      "Epoch Step:    640 | Accumulation Step:  65 | Loss:   4.36 | Tokens / Sec:   966.1 | Learning Rate: 1.7e-04\n",
      "Epoch Step:    680 | Accumulation Step:  69 | Loss:   4.11 | Tokens / Sec:   965.4 | Learning Rate: 1.8e-04\n",
      "Epoch Step:    720 | Accumulation Step:  73 | Loss:   4.17 | Tokens / Sec:   979.7 | Learning Rate: 1.9e-04\n",
      "Epoch Step:    760 | Accumulation Step:  77 | Loss:   4.03 | Tokens / Sec:   972.0 | Learning Rate: 2.0e-04\n",
      "Epoch Step:    800 | Accumulation Step:  81 | Loss:   4.04 | Tokens / Sec:   970.7 | Learning Rate: 2.2e-04\n",
      "Epoch Step:    840 | Accumulation Step:  85 | Loss:   3.93 | Tokens / Sec:   984.5 | Learning Rate: 2.3e-04\n",
      "Epoch Step:    880 | Accumulation Step:  89 | Loss:   3.78 | Tokens / Sec:   957.6 | Learning Rate: 2.4e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 96% | 80% |\n",
      "Epoch 0 Validation ====\n",
      "(tensor(3.7527, device='cuda:0'), <__main__.TrainState object at 0x000001CC9EB19650>)\n",
      "Epoch 1 Training ====\n",
      "Epoch Step:      0 | Accumulation Step:   1 | Loss:   3.87 | Tokens / Sec:  1732.8 | Learning Rate: 2.4e-04\n",
      "Epoch Step:     40 | Accumulation Step:   5 | Loss:   3.76 | Tokens / Sec:   973.9 | Learning Rate: 2.5e-04\n",
      "Epoch Step:     80 | Accumulation Step:   9 | Loss:   3.88 | Tokens / Sec:   980.6 | Learning Rate: 2.7e-04\n",
      "Epoch Step:    120 | Accumulation Step:  13 | Loss:   3.55 | Tokens / Sec:   958.2 | Learning Rate: 2.8e-04\n",
      "Epoch Step:    160 | Accumulation Step:  17 | Loss:   3.69 | Tokens / Sec:   981.8 | Learning Rate: 2.9e-04\n",
      "Epoch Step:    200 | Accumulation Step:  21 | Loss:   3.83 | Tokens / Sec:   975.2 | Learning Rate: 3.0e-04\n",
      "Epoch Step:    240 | Accumulation Step:  25 | Loss:   3.47 | Tokens / Sec:   967.9 | Learning Rate: 3.1e-04\n",
      "Epoch Step:    280 | Accumulation Step:  29 | Loss:   3.45 | Tokens / Sec:   966.3 | Learning Rate: 3.2e-04\n",
      "Epoch Step:    320 | Accumulation Step:  33 | Loss:   3.43 | Tokens / Sec:   981.2 | Learning Rate: 3.3e-04\n",
      "Epoch Step:    360 | Accumulation Step:  37 | Loss:   3.01 | Tokens / Sec:   981.8 | Learning Rate: 3.4e-04\n",
      "Epoch Step:    400 | Accumulation Step:  41 | Loss:   3.30 | Tokens / Sec:   972.8 | Learning Rate: 3.5e-04\n",
      "Epoch Step:    440 | Accumulation Step:  45 | Loss:   3.01 | Tokens / Sec:   983.8 | Learning Rate: 3.6e-04\n",
      "Epoch Step:    480 | Accumulation Step:  49 | Loss:   3.15 | Tokens / Sec:   967.0 | Learning Rate: 3.7e-04\n",
      "Epoch Step:    520 | Accumulation Step:  53 | Loss:   3.06 | Tokens / Sec:   976.9 | Learning Rate: 3.8e-04\n",
      "Epoch Step:    560 | Accumulation Step:  57 | Loss:   3.15 | Tokens / Sec:   977.6 | Learning Rate: 3.9e-04\n",
      "Epoch Step:    600 | Accumulation Step:  61 | Loss:   3.11 | Tokens / Sec:   974.8 | Learning Rate: 4.1e-04\n",
      "Epoch Step:    640 | Accumulation Step:  65 | Loss:   3.01 | Tokens / Sec:   987.6 | Learning Rate: 4.2e-04\n",
      "Epoch Step:    680 | Accumulation Step:  69 | Loss:   2.92 | Tokens / Sec:   973.2 | Learning Rate: 4.3e-04\n",
      "Epoch Step:    720 | Accumulation Step:  73 | Loss:   2.94 | Tokens / Sec:   980.2 | Learning Rate: 4.4e-04\n",
      "Epoch Step:    760 | Accumulation Step:  77 | Loss:   3.06 | Tokens / Sec:   961.2 | Learning Rate: 4.5e-04\n",
      "Epoch Step:    800 | Accumulation Step:  81 | Loss:   2.77 | Tokens / Sec:   983.8 | Learning Rate: 4.6e-04\n",
      "Epoch Step:    840 | Accumulation Step:  85 | Loss:   2.85 | Tokens / Sec:   969.6 | Learning Rate: 4.7e-04\n",
      "Epoch Step:    880 | Accumulation Step:  89 | Loss:   2.78 | Tokens / Sec:   974.8 | Learning Rate: 4.8e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 97% | 94% |\n",
      "Epoch 1 Validation ====\n",
      "(tensor(2.6127, device='cuda:0'), <__main__.TrainState object at 0x000001CC9EB19650>)\n",
      "Epoch 2 Training ====\n",
      "Epoch Step:      0 | Accumulation Step:   1 | Loss:   2.60 | Tokens / Sec:  1524.4 | Learning Rate: 4.9e-04\n",
      "Epoch Step:     40 | Accumulation Step:   5 | Loss:   2.67 | Tokens / Sec:   867.9 | Learning Rate: 5.0e-04\n",
      "Epoch Step:     80 | Accumulation Step:   9 | Loss:   2.91 | Tokens / Sec:   868.3 | Learning Rate: 5.1e-04\n",
      "Epoch Step:    120 | Accumulation Step:  13 | Loss:   2.61 | Tokens / Sec:   864.2 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    160 | Accumulation Step:  17 | Loss:   2.46 | Tokens / Sec:   860.9 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    200 | Accumulation Step:  21 | Loss:   2.43 | Tokens / Sec:   865.5 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    240 | Accumulation Step:  25 | Loss:   2.32 | Tokens / Sec:   860.0 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    280 | Accumulation Step:  29 | Loss:   2.54 | Tokens / Sec:   868.3 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    320 | Accumulation Step:  33 | Loss:   1.99 | Tokens / Sec:   874.4 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    360 | Accumulation Step:  37 | Loss:   2.36 | Tokens / Sec:   862.6 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    400 | Accumulation Step:  41 | Loss:   1.92 | Tokens / Sec:   862.2 | Learning Rate: 6.0e-04\n",
      "Epoch Step:    440 | Accumulation Step:  45 | Loss:   2.24 | Tokens / Sec:   872.4 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    480 | Accumulation Step:  49 | Loss:   2.27 | Tokens / Sec:   870.7 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    520 | Accumulation Step:  53 | Loss:   2.22 | Tokens / Sec:   858.7 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    560 | Accumulation Step:  57 | Loss:   2.04 | Tokens / Sec:   847.3 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    600 | Accumulation Step:  61 | Loss:   1.89 | Tokens / Sec:   866.0 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    640 | Accumulation Step:  65 | Loss:   2.17 | Tokens / Sec:   858.0 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    680 | Accumulation Step:  69 | Loss:   2.04 | Tokens / Sec:   871.5 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    720 | Accumulation Step:  73 | Loss:   2.01 | Tokens / Sec:   868.0 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    760 | Accumulation Step:  77 | Loss:   2.21 | Tokens / Sec:   866.1 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    800 | Accumulation Step:  81 | Loss:   1.90 | Tokens / Sec:   865.8 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    840 | Accumulation Step:  85 | Loss:   2.07 | Tokens / Sec:   861.7 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    880 | Accumulation Step:  89 | Loss:   2.18 | Tokens / Sec:   852.1 | Learning Rate: 7.2e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 97% | 93% |\n",
      "Epoch 2 Validation ====\n",
      "(tensor(1.9411, device='cuda:0'), <__main__.TrainState object at 0x000001CC9EB19650>)\n",
      "Epoch 3 Training ====\n",
      "Epoch Step:      0 | Accumulation Step:   1 | Loss:   2.00 | Tokens / Sec:  1759.7 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     40 | Accumulation Step:   5 | Loss:   1.65 | Tokens / Sec:   988.9 | Learning Rate: 7.4e-04\n",
      "Epoch Step:     80 | Accumulation Step:   9 | Loss:   1.74 | Tokens / Sec:   965.6 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    120 | Accumulation Step:  13 | Loss:   1.70 | Tokens / Sec:   978.0 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    160 | Accumulation Step:  17 | Loss:   1.84 | Tokens / Sec:   980.9 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    200 | Accumulation Step:  21 | Loss:   1.68 | Tokens / Sec:   966.3 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    240 | Accumulation Step:  25 | Loss:   1.84 | Tokens / Sec:   976.9 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    280 | Accumulation Step:  29 | Loss:   1.47 | Tokens / Sec:   977.6 | Learning Rate: 8.1e-04\n",
      "Epoch Step:    320 | Accumulation Step:  33 | Loss:   1.59 | Tokens / Sec:   989.0 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    360 | Accumulation Step:  37 | Loss:   1.92 | Tokens / Sec:   985.0 | Learning Rate: 8.0e-04\n",
      "Epoch Step:    400 | Accumulation Step:  41 | Loss:   1.72 | Tokens / Sec:   991.4 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    440 | Accumulation Step:  45 | Loss:   1.31 | Tokens / Sec:   974.0 | Learning Rate: 7.9e-04\n",
      "Epoch Step:    480 | Accumulation Step:  49 | Loss:   1.80 | Tokens / Sec:   970.4 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    520 | Accumulation Step:  53 | Loss:   1.45 | Tokens / Sec:   969.7 | Learning Rate: 7.8e-04\n",
      "Epoch Step:    560 | Accumulation Step:  57 | Loss:   1.70 | Tokens / Sec:   991.0 | Learning Rate: 7.7e-04\n",
      "Epoch Step:    600 | Accumulation Step:  61 | Loss:   1.80 | Tokens / Sec:   981.0 | Learning Rate: 7.7e-04\n",
      "Epoch Step:    640 | Accumulation Step:  65 | Loss:   1.98 | Tokens / Sec:   972.4 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    680 | Accumulation Step:  69 | Loss:   1.57 | Tokens / Sec:   976.2 | Learning Rate: 7.6e-04\n",
      "Epoch Step:    720 | Accumulation Step:  73 | Loss:   1.85 | Tokens / Sec:   980.5 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    760 | Accumulation Step:  77 | Loss:   1.53 | Tokens / Sec:   988.4 | Learning Rate: 7.5e-04\n",
      "Epoch Step:    800 | Accumulation Step:  81 | Loss:   2.02 | Tokens / Sec:   970.1 | Learning Rate: 7.4e-04\n",
      "Epoch Step:    840 | Accumulation Step:  85 | Loss:   1.57 | Tokens / Sec:   972.5 | Learning Rate: 7.4e-04\n",
      "Epoch Step:    880 | Accumulation Step:  89 | Loss:   1.86 | Tokens / Sec:   976.4 | Learning Rate: 7.4e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 97% | 93% |\n",
      "Epoch 3 Validation ====\n",
      "(tensor(1.6398, device='cuda:0'), <__main__.TrainState object at 0x000001CC9EB19650>)\n",
      "Epoch 4 Training ====\n",
      "Epoch Step:      0 | Accumulation Step:   1 | Loss:   1.28 | Tokens / Sec:  1674.1 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     40 | Accumulation Step:   5 | Loss:   1.51 | Tokens / Sec:   967.2 | Learning Rate: 7.3e-04\n",
      "Epoch Step:     80 | Accumulation Step:   9 | Loss:   1.42 | Tokens / Sec:   979.2 | Learning Rate: 7.3e-04\n",
      "Epoch Step:    120 | Accumulation Step:  13 | Loss:   1.14 | Tokens / Sec:   987.3 | Learning Rate: 7.2e-04\n",
      "Epoch Step:    160 | Accumulation Step:  17 | Loss:   1.20 | Tokens / Sec:   975.9 | Learning Rate: 7.2e-04\n",
      "Epoch Step:    200 | Accumulation Step:  21 | Loss:   1.35 | Tokens / Sec:   980.3 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    240 | Accumulation Step:  25 | Loss:   1.51 | Tokens / Sec:   969.3 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    280 | Accumulation Step:  29 | Loss:   1.22 | Tokens / Sec:   972.3 | Learning Rate: 7.1e-04\n",
      "Epoch Step:    320 | Accumulation Step:  33 | Loss:   1.55 | Tokens / Sec:   968.2 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    360 | Accumulation Step:  37 | Loss:   1.36 | Tokens / Sec:   982.7 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    400 | Accumulation Step:  41 | Loss:   1.14 | Tokens / Sec:   975.1 | Learning Rate: 7.0e-04\n",
      "Epoch Step:    440 | Accumulation Step:  45 | Loss:   1.28 | Tokens / Sec:   981.6 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    480 | Accumulation Step:  49 | Loss:   1.40 | Tokens / Sec:   982.4 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    520 | Accumulation Step:  53 | Loss:   1.27 | Tokens / Sec:   970.0 | Learning Rate: 6.9e-04\n",
      "Epoch Step:    560 | Accumulation Step:  57 | Loss:   1.37 | Tokens / Sec:   986.0 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    600 | Accumulation Step:  61 | Loss:   1.30 | Tokens / Sec:   995.8 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    640 | Accumulation Step:  65 | Loss:   1.40 | Tokens / Sec:   975.8 | Learning Rate: 6.8e-04\n",
      "Epoch Step:    680 | Accumulation Step:  69 | Loss:   1.33 | Tokens / Sec:   981.4 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    720 | Accumulation Step:  73 | Loss:   1.43 | Tokens / Sec:   968.8 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    760 | Accumulation Step:  77 | Loss:   1.26 | Tokens / Sec:   986.0 | Learning Rate: 6.7e-04\n",
      "Epoch Step:    800 | Accumulation Step:  81 | Loss:   1.42 | Tokens / Sec:   974.1 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    840 | Accumulation Step:  85 | Loss:   1.30 | Tokens / Sec:   985.0 | Learning Rate: 6.6e-04\n",
      "Epoch Step:    880 | Accumulation Step:  89 | Loss:   1.16 | Tokens / Sec:   978.8 | Learning Rate: 6.6e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 96% | 93% |\n",
      "Epoch 4 Validation ====\n",
      "(tensor(1.5017, device='cuda:0'), <__main__.TrainState object at 0x000001CC9EB19650>)\n",
      "Epoch 5 Training ====\n",
      "Epoch Step:      0 | Accumulation Step:   1 | Loss:   1.12 | Tokens / Sec:  1680.9 | Learning Rate: 6.6e-04\n",
      "Epoch Step:     40 | Accumulation Step:   5 | Loss:   1.09 | Tokens / Sec:   973.5 | Learning Rate: 6.5e-04\n",
      "Epoch Step:     80 | Accumulation Step:   9 | Loss:   1.13 | Tokens / Sec:   982.6 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    120 | Accumulation Step:  13 | Loss:   0.93 | Tokens / Sec:   969.2 | Learning Rate: 6.5e-04\n",
      "Epoch Step:    160 | Accumulation Step:  17 | Loss:   1.23 | Tokens / Sec:   957.6 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    200 | Accumulation Step:  21 | Loss:   1.21 | Tokens / Sec:   971.1 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    240 | Accumulation Step:  25 | Loss:   1.22 | Tokens / Sec:   977.1 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    280 | Accumulation Step:  29 | Loss:   1.19 | Tokens / Sec:   993.0 | Learning Rate: 6.4e-04\n",
      "Epoch Step:    320 | Accumulation Step:  33 | Loss:   0.99 | Tokens / Sec:   974.9 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    360 | Accumulation Step:  37 | Loss:   1.25 | Tokens / Sec:   974.6 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    400 | Accumulation Step:  41 | Loss:   0.95 | Tokens / Sec:   990.8 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    440 | Accumulation Step:  45 | Loss:   1.26 | Tokens / Sec:   983.9 | Learning Rate: 6.3e-04\n",
      "Epoch Step:    480 | Accumulation Step:  49 | Loss:   1.19 | Tokens / Sec:  1003.4 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    520 | Accumulation Step:  53 | Loss:   0.95 | Tokens / Sec:   992.6 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    560 | Accumulation Step:  57 | Loss:   1.20 | Tokens / Sec:   991.5 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    600 | Accumulation Step:  61 | Loss:   1.26 | Tokens / Sec:   970.8 | Learning Rate: 6.2e-04\n",
      "Epoch Step:    640 | Accumulation Step:  65 | Loss:   0.94 | Tokens / Sec:   972.0 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    680 | Accumulation Step:  69 | Loss:   1.18 | Tokens / Sec:   977.1 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    720 | Accumulation Step:  73 | Loss:   1.15 | Tokens / Sec:   970.8 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    760 | Accumulation Step:  77 | Loss:   1.17 | Tokens / Sec:   983.9 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    800 | Accumulation Step:  81 | Loss:   1.23 | Tokens / Sec:   989.7 | Learning Rate: 6.1e-04\n",
      "Epoch Step:    840 | Accumulation Step:  85 | Loss:   1.20 | Tokens / Sec:   993.4 | Learning Rate: 6.0e-04\n",
      "Epoch Step:    880 | Accumulation Step:  89 | Loss:   1.07 | Tokens / Sec:   970.4 | Learning Rate: 6.0e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 97% | 93% |\n",
      "Epoch 5 Validation ====\n",
      "(tensor(1.4472, device='cuda:0'), <__main__.TrainState object at 0x000001CC9EB19650>)\n",
      "Epoch 6 Training ====\n",
      "Epoch Step:      0 | Accumulation Step:   1 | Loss:   0.94 | Tokens / Sec:  1547.0 | Learning Rate: 6.0e-04\n",
      "Epoch Step:     40 | Accumulation Step:   5 | Loss:   0.83 | Tokens / Sec:   935.4 | Learning Rate: 6.0e-04\n",
      "Epoch Step:     80 | Accumulation Step:   9 | Loss:   0.80 | Tokens / Sec:   952.2 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    120 | Accumulation Step:  13 | Loss:   0.84 | Tokens / Sec:   950.6 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    160 | Accumulation Step:  17 | Loss:   1.03 | Tokens / Sec:   942.8 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    200 | Accumulation Step:  21 | Loss:   0.89 | Tokens / Sec:   926.6 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    240 | Accumulation Step:  25 | Loss:   0.83 | Tokens / Sec:   970.7 | Learning Rate: 5.9e-04\n",
      "Epoch Step:    280 | Accumulation Step:  29 | Loss:   1.07 | Tokens / Sec:   961.5 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    320 | Accumulation Step:  33 | Loss:   0.82 | Tokens / Sec:   953.7 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    360 | Accumulation Step:  37 | Loss:   0.93 | Tokens / Sec:   951.5 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    400 | Accumulation Step:  41 | Loss:   0.95 | Tokens / Sec:   945.8 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    440 | Accumulation Step:  45 | Loss:   0.86 | Tokens / Sec:   959.5 | Learning Rate: 5.8e-04\n",
      "Epoch Step:    480 | Accumulation Step:  49 | Loss:   0.97 | Tokens / Sec:   951.3 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    520 | Accumulation Step:  53 | Loss:   0.86 | Tokens / Sec:   936.7 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    560 | Accumulation Step:  57 | Loss:   0.78 | Tokens / Sec:   952.5 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    600 | Accumulation Step:  61 | Loss:   1.06 | Tokens / Sec:   958.2 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    640 | Accumulation Step:  65 | Loss:   1.05 | Tokens / Sec:   951.5 | Learning Rate: 5.7e-04\n",
      "Epoch Step:    680 | Accumulation Step:  69 | Loss:   0.87 | Tokens / Sec:   943.2 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    720 | Accumulation Step:  73 | Loss:   0.74 | Tokens / Sec:   946.9 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    760 | Accumulation Step:  77 | Loss:   1.11 | Tokens / Sec:   946.7 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    800 | Accumulation Step:  81 | Loss:   1.11 | Tokens / Sec:   951.6 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    840 | Accumulation Step:  85 | Loss:   0.90 | Tokens / Sec:   958.9 | Learning Rate: 5.6e-04\n",
      "Epoch Step:    880 | Accumulation Step:  89 | Loss:   0.91 | Tokens / Sec:   940.2 | Learning Rate: 5.6e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 97% | 93% |\n",
      "Epoch 6 Validation ====\n",
      "(tensor(1.4420, device='cuda:0'), <__main__.TrainState object at 0x000001CC9EB19650>)\n",
      "Epoch 7 Training ====\n",
      "Epoch Step:      0 | Accumulation Step:   1 | Loss:   0.83 | Tokens / Sec:  1599.9 | Learning Rate: 5.5e-04\n",
      "Epoch Step:     40 | Accumulation Step:   5 | Loss:   0.92 | Tokens / Sec:   966.8 | Learning Rate: 5.5e-04\n",
      "Epoch Step:     80 | Accumulation Step:   9 | Loss:   0.79 | Tokens / Sec:   989.1 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    120 | Accumulation Step:  13 | Loss:   0.82 | Tokens / Sec:   993.3 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    160 | Accumulation Step:  17 | Loss:   0.85 | Tokens / Sec:   977.1 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    200 | Accumulation Step:  21 | Loss:   0.75 | Tokens / Sec:   983.0 | Learning Rate: 5.5e-04\n",
      "Epoch Step:    240 | Accumulation Step:  25 | Loss:   0.70 | Tokens / Sec:   983.5 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    280 | Accumulation Step:  29 | Loss:   0.82 | Tokens / Sec:   973.9 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    320 | Accumulation Step:  33 | Loss:   0.91 | Tokens / Sec:   966.5 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    360 | Accumulation Step:  37 | Loss:   0.65 | Tokens / Sec:   982.3 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    400 | Accumulation Step:  41 | Loss:   0.79 | Tokens / Sec:   959.4 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    440 | Accumulation Step:  45 | Loss:   0.73 | Tokens / Sec:   979.4 | Learning Rate: 5.4e-04\n",
      "Epoch Step:    480 | Accumulation Step:  49 | Loss:   0.94 | Tokens / Sec:   970.3 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    520 | Accumulation Step:  53 | Loss:   0.85 | Tokens / Sec:   994.0 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    560 | Accumulation Step:  57 | Loss:   0.75 | Tokens / Sec:   980.5 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    600 | Accumulation Step:  61 | Loss:   0.65 | Tokens / Sec:   977.4 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    640 | Accumulation Step:  65 | Loss:   0.82 | Tokens / Sec:   994.7 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    680 | Accumulation Step:  69 | Loss:   0.87 | Tokens / Sec:   982.4 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    720 | Accumulation Step:  73 | Loss:   0.95 | Tokens / Sec:   978.2 | Learning Rate: 5.3e-04\n",
      "Epoch Step:    760 | Accumulation Step:  77 | Loss:   0.83 | Tokens / Sec:   974.2 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    800 | Accumulation Step:  81 | Loss:   0.77 | Tokens / Sec:   985.4 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    840 | Accumulation Step:  85 | Loss:   0.98 | Tokens / Sec:   984.4 | Learning Rate: 5.2e-04\n",
      "Epoch Step:    880 | Accumulation Step:  89 | Loss:   0.88 | Tokens / Sec:   974.0 | Learning Rate: 5.2e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 97% | 92% |\n",
      "Epoch 7 Validation ====\n",
      "(tensor(1.4482, device='cuda:0'), <__main__.TrainState object at 0x000001CC9EB19650>)\n"
     ]
    }
   ],
   "source": [
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(model, models):\n",
    "    \"Average models into model\"\n",
    "    print(isinstance(model, torch.nn.Module))  # Should print True\n",
    "    print(all(isinstance(m, torch.nn.Module) for m in models))  # Check all models\n",
    "    for p_target, *p_sources in zip(model.parameters(), *[m.parameters() for m in models]):\n",
    "        p_target.data.copy_(torch.sum(torch.stack(p_sources), dim=0) / len(p_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "model = make_model(config).to(device)\n",
    "model.load_state_dict(torch.load(\"multi30k_model_final.pt\"))\n",
    "models = [make_model(config).to(device) for _ in range(3)]\n",
    "\n",
    "# Load state dictionaries separately\n",
    "for version in range(3):\n",
    "    models[version].load_state_dict(torch.load(f\"multi30k_model_0{7-version}.pt\"))\n",
    "\n",
    "average(model, models)\n",
    "torch.save(model.state_dict(), \"multi30k_model_final_avg.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_outputs(\n",
    "    valid_dataloader,\n",
    "    model,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    n_examples=15,\n",
    "    pad_idx=2,\n",
    "    eos_string=\"</s>\",\n",
    "):\n",
    "    results = [()] * n_examples\n",
    "    for idx in range(n_examples):\n",
    "        print(\"\\nExample %d ========\\n\" % idx)\n",
    "        b = next(iter(valid_dataloader))\n",
    "        rb = Batch(b[0], b[1], pad_idx)\n",
    "        model.generate(rb.src, rb.src_mask, 64, 0)[0]\n",
    "\n",
    "        src_tokens = [\n",
    "            vocab_src.get_itos()[x] for x in rb.src[0] if x != pad_idx\n",
    "        ]\n",
    "        tgt_tokens = [\n",
    "            vocab_tgt.get_itos()[x] for x in rb.tgt[0] if x != pad_idx\n",
    "        ]\n",
    "\n",
    "        print(\n",
    "            \"Source Text (Input)        : \"\n",
    "            + \" \".join(src_tokens).replace(\"\\n\", \"\")\n",
    "        )\n",
    "        print(\n",
    "            \"Target Text (Ground Truth) : \"\n",
    "            + \" \".join(tgt_tokens).replace(\"\\n\", \"\")\n",
    "        )\n",
    "        model_out = model.generate(rb.src, rb.src_mask, 72, 0)[0]\n",
    "        model_txt = (\n",
    "            \" \".join(\n",
    "                [vocab_tgt.get_itos()[x] for x in model_out if x != pad_idx]\n",
    "            ).split(eos_string, 1)[0]\n",
    "            + eos_string\n",
    "        )\n",
    "        print(\"Model Output               : \" + model_txt.replace(\"\\n\", \"\"))\n",
    "        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_example(config, n_examples=5):\n",
    "    global vocab_src, vocab_tgt, spacy_de, spacy_en\n",
    "\n",
    "    print(\"Preparing Data ...\")\n",
    "    _, valid_dataloader = create_dataloaders(\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        batch_size=1,\n",
    "        is_distributed=False,\n",
    "        device='cuda'\n",
    "    )\n",
    "\n",
    "    print(\"Loading Trained Model ...\")\n",
    "\n",
    "    model = make_model(config)\n",
    "    model.load_state_dict(torch.load(\"multi30k_model_final_avg.pt\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    print(\"Checking Model Outputs:\")\n",
    "    example_data = check_outputs(valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples)\n",
    "    return model, example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Data ...\n",
      "Loading Trained Model ...\n",
      "Checking Model Outputs:\n",
      "\n",
      "Example 0 ========\n",
      "\n",
      "Source Text (Input)        : <s> Ein Mann an einem Strand baut eine Sandburg . </s>\n",
      "Target Text (Ground Truth) : <s> A man on a beach building a sand castle . </s>\n",
      "Model Output               : <s> A man on a beach building a sand castle . </s>\n",
      "\n",
      "Example 1 ========\n",
      "\n",
      "Source Text (Input)        : <s> Ein Mann mit beginnender Glatze , der eine rote Rettungsweste trägt , sitzt in einem kleinen Boot . </s>\n",
      "Target Text (Ground Truth) : <s> A balding man wearing a red life jacket is sitting in a small boat . </s>\n",
      "Model Output               : <s> A balding man wearing a red life jacket sitting in a small boat . </s>\n",
      "\n",
      "Example 2 ========\n",
      "\n",
      "Source Text (Input)        : <s> Ein Mann führt zwei kleine <unk> in einem Park spazieren . </s>\n",
      "Target Text (Ground Truth) : <s> A man is leading two small <unk> on a walk at a park . </s>\n",
      "Model Output               : <s> A man walks two small <unk> in a park . </s>\n",
      "\n",
      "Example 3 ========\n",
      "\n",
      "Source Text (Input)        : <s> Zwei Männer hinter einer kreisförmigen Bar in einem Raum voll mit Menschen . </s>\n",
      "Target Text (Ground Truth) : <s> Two men behind a circular bar in a room filled with people . </s>\n",
      "Model Output               : <s> Two men behind a circular bar in a room full of people . </s>\n",
      "\n",
      "Example 4 ========\n",
      "\n",
      "Source Text (Input)        : <s> Ein lächelnder junger Mann geht in der Nähe des Strandes vorbei und trägt dabei eine Baseballkappe , ein blaues T-Shirt und Jeans . </s>\n",
      "Target Text (Ground Truth) : <s> A smiling young man walking on next to the beach wearing a baseball cap , blue t - shirt and jeans . </s>\n",
      "Model Output               : <s> A smiling young man walks by the beach wearing a blue t - shirt and blue t - shirt and jeans . </s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(EncoderDecoder(\n",
       "   (encoder): Encoder(\n",
       "     (layers): ModuleList(\n",
       "       (0-5): 6 x EncoderLayer(\n",
       "         (self_attn): MultiHeadedAttention(\n",
       "           (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (feed_forward): FeedForward(\n",
       "           (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (sublayer): ModuleList(\n",
       "           (0-1): 2 x SublayerConnection(\n",
       "             (norm): LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (norm): LayerNorm()\n",
       "   )\n",
       "   (decoder): Decoder(\n",
       "     (layers): ModuleList(\n",
       "       (0-5): 6 x DecoderLayer(\n",
       "         (s_attn): MultiHeadedAttention(\n",
       "           (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (x_attn): MultiHeadedAttention(\n",
       "           (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (feed_forward): FeedForward(\n",
       "           (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "           (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           (dropout): Dropout(p=0.1, inplace=False)\n",
       "         )\n",
       "         (sublayer): ModuleList(\n",
       "           (0-2): 3 x SublayerConnection(\n",
       "             (norm): LayerNorm()\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (norm): LayerNorm()\n",
       "   )\n",
       "   (proj): Linear(in_features=512, out_features=6291, bias=True)\n",
       "   (src_embed): Embeddings(\n",
       "     (token_embeddings): Embedding(8185, 512)\n",
       "     (position_embeddings): Embedding(5000, 512)\n",
       "     (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       "   (tgt_embed): Embeddings(\n",
       "     (token_embeddings): Embedding(6291, 512)\n",
       "     (position_embeddings): Embedding(5000, 512)\n",
       "     (layer_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "   )\n",
       " ),\n",
       " [(<Processing.batch.Batch at 0x1cd0e45b050>,\n",
       "   ['<s>',\n",
       "    'Ein',\n",
       "    'Mann',\n",
       "    'an',\n",
       "    'einem',\n",
       "    'Strand',\n",
       "    'baut',\n",
       "    'eine',\n",
       "    'Sandburg',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   ['<s>',\n",
       "    'A',\n",
       "    'man',\n",
       "    'on',\n",
       "    'a',\n",
       "    'beach',\n",
       "    'building',\n",
       "    'a',\n",
       "    'sand',\n",
       "    'castle',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   tensor([   0,    6,   12,    9,    4,   91,   80,    4,  212, 1528,    5,    1,\n",
       "              1,    5,    1,    1,    1,    5,    1,    1,    5,    1,    5,    1,\n",
       "              1,    1,    5,    1,    5,    1,    1,    5,    1,    5,    1,    1,\n",
       "              1,    1,    1,    1,    1,    5,    1,    1,    9,    4,  212,    5,\n",
       "              1,    5,    1,    1,    1,    1,    1,    5,    1,    1,    5,    1,\n",
       "              1,    5,    1,    1,    9,    4,  212,    5,    1,    1,    1,    1],\n",
       "          device='cuda:0'),\n",
       "   '<s> A man on a beach building a sand castle . </s>'),\n",
       "  (<Processing.batch.Batch at 0x1cd0e4cfdd0>,\n",
       "   ['<s>',\n",
       "    'Ein',\n",
       "    'Mann',\n",
       "    'mit',\n",
       "    'beginnender',\n",
       "    'Glatze',\n",
       "    ',',\n",
       "    'der',\n",
       "    'eine',\n",
       "    'rote',\n",
       "    'Rettungsweste',\n",
       "    'trägt',\n",
       "    ',',\n",
       "    'sitzt',\n",
       "    'in',\n",
       "    'einem',\n",
       "    'kleinen',\n",
       "    'Boot',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   ['<s>',\n",
       "    'A',\n",
       "    'balding',\n",
       "    'man',\n",
       "    'wearing',\n",
       "    'a',\n",
       "    'red',\n",
       "    'life',\n",
       "    'jacket',\n",
       "    'is',\n",
       "    'sitting',\n",
       "    'in',\n",
       "    'a',\n",
       "    'small',\n",
       "    'boat',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   tensor([   0,    6, 1402,   12,   21,    4,   31,  726,   86,   32,    7,    4,\n",
       "             77,  182,    5,    1,    1,    5,    1,    1,    5,    1,    1,    5,\n",
       "              1,    1,    5,    1,    5,    1,    5,    1,    1,    5,    1,    1,\n",
       "              5,    1,    1,    1,    5,    1,    1,    1,    5,    1,    1,    1,\n",
       "              1,    5,    1,    1,    1,    1,    1,   21,    4,   77,    4,   77,\n",
       "             21,    4,   77,    4,   77,  182,    5,    1,    1,    1,    1,    1],\n",
       "          device='cuda:0'),\n",
       "   '<s> A balding man wearing a red life jacket sitting in a small boat . </s>'),\n",
       "  (<Processing.batch.Batch at 0x1cd0fc7be10>,\n",
       "   ['<s>',\n",
       "    'Ein',\n",
       "    'Mann',\n",
       "    'führt',\n",
       "    'zwei',\n",
       "    'kleine',\n",
       "    '<unk>',\n",
       "    'in',\n",
       "    'einem',\n",
       "    'Park',\n",
       "    'spazieren',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   ['<s>',\n",
       "    'A',\n",
       "    'man',\n",
       "    'is',\n",
       "    'leading',\n",
       "    'two',\n",
       "    'small',\n",
       "    '<unk>',\n",
       "    'on',\n",
       "    'a',\n",
       "    'walk',\n",
       "    'at',\n",
       "    'a',\n",
       "    'park',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   tensor([  0,   6,  12, 125,  66,  77,   3,   7,   4, 121,   5,   1,   1,   5,\n",
       "             1,   5,   1,   1,   1,   5,   1,   5,   1,   5,   1,   1,   5,   1,\n",
       "             1,   1,   5,   1,   1,   5,   1,   1,   5,   1,   1,   1,   1,   1,\n",
       "             1,   5,   1,   5,   1,   1,   1,   5,   1,  66,  77,   3,   1,   5,\n",
       "             1,   5,   1,   1,   1,   1,   1,   1,   5,   1,   5,   1,   1,   5,\n",
       "             1,   1], device='cuda:0'),\n",
       "   '<s> A man walks two small <unk> in a park . </s>'),\n",
       "  (<Processing.batch.Batch at 0x1cd0e4cf010>,\n",
       "   ['<s>',\n",
       "    'Zwei',\n",
       "    'Männer',\n",
       "    'hinter',\n",
       "    'einer',\n",
       "    'kreisförmigen',\n",
       "    'Bar',\n",
       "    'in',\n",
       "    'einem',\n",
       "    'Raum',\n",
       "    'voll',\n",
       "    'mit',\n",
       "    'Menschen',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   ['<s>',\n",
       "    'Two',\n",
       "    'men',\n",
       "    'behind',\n",
       "    'a',\n",
       "    'circular',\n",
       "    'bar',\n",
       "    'in',\n",
       "    'a',\n",
       "    'room',\n",
       "    'filled',\n",
       "    'with',\n",
       "    'people',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   tensor([   0,   19,   36,   98,    4, 1956,  368,    7,    4,  187,  304,   13,\n",
       "             22,    5,    1,    5,    1,    5,    1,    1,    5,    1,    5,    1,\n",
       "              5,    1,    5,    1,    5,    1,    5,    1,    1,    5,    1,    1,\n",
       "             98,   22,    1,    1,    5,    1,    1,   14,   22,    5,    1,   98,\n",
       "            160,    5,    1,    1,    1,    1,    1,  304,   13,   22,    5,    1,\n",
       "             14,   22,   98,    4,  368,    5,    1,   13,   22,    5,    1,    1],\n",
       "          device='cuda:0'),\n",
       "   '<s> Two men behind a circular bar in a room full of people . </s>'),\n",
       "  (<Processing.batch.Batch at 0x1cd0e4cf550>,\n",
       "   ['<s>',\n",
       "    'Ein',\n",
       "    'lächelnder',\n",
       "    'junger',\n",
       "    'Mann',\n",
       "    'geht',\n",
       "    'in',\n",
       "    'der',\n",
       "    'Nähe',\n",
       "    'des',\n",
       "    'Strandes',\n",
       "    'vorbei',\n",
       "    'und',\n",
       "    'trägt',\n",
       "    'dabei',\n",
       "    'eine',\n",
       "    'Baseballkappe',\n",
       "    ',',\n",
       "    'ein',\n",
       "    'blaues',\n",
       "    'T-Shirt',\n",
       "    'und',\n",
       "    'Jeans',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   ['<s>',\n",
       "    'A',\n",
       "    'smiling',\n",
       "    'young',\n",
       "    'man',\n",
       "    'walking',\n",
       "    'on',\n",
       "    'next',\n",
       "    'to',\n",
       "    'the',\n",
       "    'beach',\n",
       "    'wearing',\n",
       "    'a',\n",
       "    'baseball',\n",
       "    'cap',\n",
       "    ',',\n",
       "    'blue',\n",
       "    't',\n",
       "    '-',\n",
       "    'shirt',\n",
       "    'and',\n",
       "    'jeans',\n",
       "    '.',\n",
       "    '</s>'],\n",
       "   tensor([  0,   6, 134,  25,  12, 125,  48,   8,  91,  21,   4,  30, 217,  43,\n",
       "            23,  11,  30, 217,  43,  23,  11, 177,   5,   1,   1,   5,   1,   1,\n",
       "             5,   1,   1,   5,   1,   5,   1,   1,   5,   1,   1,   5,   1,   5,\n",
       "             1,  42,  48,   8,  91,  84,   8,  91,  84,   8,  91,  15,  21,   4,\n",
       "            30,  23,  11,  30, 217,   1,  43,  23,  11,  30, 217,  43,  23,  15,\n",
       "            21,   4], device='cuda:0'),\n",
       "   '<s> A smiling young man walks by the beach wearing a blue t - shirt and blue t - shirt and jeans . </s>')])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config()\n",
    "config.batch_size = 32\n",
    "config.src_vocab = len(vocab_src)\n",
    "config.tgt_vocab = len(vocab_tgt)\n",
    "config.N = 6\n",
    "run_model_example(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
